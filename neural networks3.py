# -*- coding: utf-8 -*-
"""КП№3 Ащимов Арсеній БС-14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QnjtnHI_xQEbWFnI3doJ2qm6QOnP3dHm

# 1. Завантажити датасет відповідно до варіанту завдання. Застосувати до нього розроблену нейронну мережу. Зробити за необхідністю попередню обробку даних.
"""

import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier

# Завантажте набір даних Digits
digits = datasets.load_digits()
X_digits = digits.data
Y_digits = digits.target.reshape(-1, 1)
scaler_digits = StandardScaler().fit(X_digits)
X_digits = scaler_digits.transform(X_digits)

X_digits_train, X_digits_test, Y_digits_train, Y_digits_test = train_test_split(X_digits, Y_digits, test_size=0.2,
                                                                                random_state=42)


def initialize_weights(n_input, n_hidden, n_output):
    W1 = np.random.randn(n_input, n_hidden) * 0.01
    b1 = np.zeros((1, n_hidden))
    W2 = np.random.randn(n_hidden, n_output) * 0.01
    b2 = np.zeros((1, n_output))

    return W1, b1, W2, b2


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def sigmoid_derivative(x):
    return x * (1 - x)


def initialize_weights(n_input, n_hidden, n_output):
    W1 = np.random.randn(n_input, n_hidden) * 0.01
    b1 = np.zeros((1, n_hidden))
    W2 = np.random.randn(n_hidden, n_output) * 0.01
    b2 = np.zeros((1, n_output))
    return W1, b1, W2, b2


def forward_pass_multiclass(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    return Z1, A1, Z2, A2


def backward_pass_multiclass(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate=0.01):
    m = X.shape[0]
    dZ2 = A2 - Y
    dW2 = 1 / m * np.dot(A1.T, dZ2)
    db2 = 1 / m * np.sum(dZ2, axis=0, keepdims=True)
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * sigmoid_derivative(A1)
    dW1 = 1 / m * np.dot(X.T, dZ1)
    db1 = 1 / m * np.sum(dZ1, axis=0, keepdims=True)
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    return W1, b1, W2, b2


def compute_multiclass_loss(Y, Y_hat):
    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[0]
    L = -(1 / m) * L_sum
    return L


def train_model(X, Y, W1, b1, W2, b2, num_iterations=1000):
    loss_values = []
    for i in range(num_iterations):
        Z1, A1, Z2, A2 = forward_pass_multiclass(X, W1, b1, W2, b2)
        W1, b1, W2, b2 = backward_pass_multiclass(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate=0.01)
        loss = compute_multiclass_loss(Y, A2)
        loss_values.append(loss)
    return W1, b1, W2, b2, loss_values


W1, b1, W2, b2 = initialize_weights(X_digits_train.shape[1], n_hidden=5, n_output=1)
W1, b1, W2, b2, loss_values = train_model(X_digits_train, Y_digits_train, W1, b1, W2, b2, num_iterations=1000)

print(loss_values)
plt.plot(loss_values)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Training Loss for Digits Dataset')
plt.show()

"""# 2. Порівняння з готовими рішеннями та проаналізувати результати роботи, розрахувавши метрики якості. Використати готову модель нейронної мережі з бібліотеки scikit-learn і порівняти її роботу з розробленою вручну мережею, наприклад для MLPClassifier:

"""

def accuracy(y_true, y_pred):
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return correct / total


def precision(y_true, y_pred):
    true_positive = np.sum(np.logical_and(y_true == 1, y_pred == 1))
    false_positive = np.sum(np.logical_and(y_true == 0, y_pred == 1))
    return true_positive / (true_positive + false_positive)


def recall(y_true, y_pred):
    true_positive = np.sum(np.logical_and(y_true == 1, y_pred == 1))
    false_negative = np.sum(np.logical_and(y_true == 1, y_pred == 0))
    return true_positive / (true_positive + false_negative)


def f1_score(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return 2 * (p * r) / (p + r)


Z1_test = np.dot(X_digits_test, W1) + b1
A1_test = np.tanh(Z1_test)
Z2_test = np.dot(A1_test, W2) + b2
A2_test = 1 / (1 + np.exp(-Z2_test))
predictions = (A2_test > 0.5).astype(int)


accuracy = accuracy(Y_digits_test, predictions)
sensitivity = precision(Y_digits_test, predictions)
specificity = recall(Y_digits_test, predictions)
f1_score = f1_score(Y_digits_test, predictions)

print("Accuracy:", accuracy)
print("Precision:", sensitivity)
print("Recall:", specificity)
print("F1-score:", f1_score, "\n")

# MLPClassifier
n_hidden = 50
clf = MLPClassifier(hidden_layer_sizes=(n_hidden,), max_iter=400, alpha=0.01, solver='sgd', verbose=10, random_state=42,
                    learning_rate_init=.01)
clf.fit(X_digits_train, Y_digits_train.ravel())
Y_digits_s = clf.predict(X_digits_test)

"""# 3. Оптимізація архітектури нейронної мережі. Спробувати змінити архітектуру мережі (додати більше прихованих шарів, змінити функції активації) і подивитися, як це вплине на результати. Проаналізувати результат."""

import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier


digits = datasets.load_digits()
X_digits = digits.data
Y_digits = digits.target.reshape(-1, 1)
scaler_digits = StandardScaler().fit(X_digits)
X_digits = scaler_digits.transform(X_digits)

X_digits_train, X_digits_test, Y_digits_train, Y_digits_test = train_test_split(X_digits, Y_digits, test_size=0.2,
                                                                                random_state=42)


def initialize_weights(n_input, n_hidden, n_output):
    W1 = np.random.randn(n_input, n_hidden) * 0.01
    b1 = np.zeros((1, n_hidden))
    W2 = np.random.randn(n_hidden, n_output) * 0.01
    b2 = np.zeros((1, n_output))

    return W1, b1, W2, b2


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def sigmoid_derivative(x):
    return x * (1 - x)

def initialize_weights_modified(n_input, n_hidden1, n_hidden2, n_output):
    W1 = np.random.randn(n_input, n_hidden1) * 0.01
    b1 = np.zeros((1, n_hidden1))
    W2 = np.random.randn(n_hidden1, n_hidden2) * 0.01
    b2 = np.zeros((1, n_hidden2))
    W3 = np.random.randn(n_hidden2, n_output) * 0.01
    b3 = np.zeros((1, n_output))
    return W1, b1, W2, b2, W3, b3


def forward_pass_multiclass(X, W1, b1, W2, b2, W3, b3):
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    Z3 = np.dot(A2, W3) + b3
    A3 = np.tanh(Z3)
    return Z1, A1, Z2, A2, Z3, A3


def backward_pass_multiclass(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, b1, b2, W3, b3, learning_rate=0.01):
    m = X.shape[0]
    dZ3 = A3 - Y
    dW3 = 1 / m * np.dot(A2.T, dZ3)
    db3 = 1 / m * np.sum(dZ3, axis=0, keepdims=True)
    dA2 = np.dot(dZ3, W3.T)

    dZ2 = dA2 * (Z2 > 0)
    dW2 = 1 / m * np.dot(A1.T, dZ2)
    db2 = 1 / m * np.sum(dZ2, axis=0, keepdims=True)
    dA1 = np.dot(dZ2, W2.T)


    dZ1 = dA1 * (Z1 > 0)
    dW1 = 1 / m * np.dot(X.T, dZ1)
    db1 = 1 / m * np.sum(dZ1, axis=0, keepdims=True)

    W3 -= learning_rate * dW3
    b3 -= learning_rate * db3
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1

    return W1, b1, W2, b2, W3, b3



def compute_multiclass_loss(Y, Y_hat):
    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[0]
    L = -(1 / m) * L_sum
    return L


def train_model_modified(X, Y, W1, b1, W2, b2, W3, b3, num_iterations=1000, learning_rate=0.01):
    loss_values = []

    for i in range(num_iterations):
        Z1, A1, Z2, A2, Z3, A3 = forward_pass_multiclass(X, W1, b1, W2, b2, W3, b3)
        W1, b1, W2, b2, W3, b3 = backward_pass_multiclass(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, b1, b2, W3, b3, learning_rate)

        loss = compute_multiclass_loss(Y, A3)
        loss_values.append(loss)

    return W1, b1, W2, b2, W3, b3, loss_values

n_hidden1 = 64
n_hidden2 = 32
W1, b1, W2, b2, W3, b3 = initialize_weights_modified(X_digits_train.shape[1], n_hidden1, n_hidden2, n_output=1)
W1, b1, W2, b2, W3, b3, loss_values_modified = train_model_modified(X_digits_train, Y_digits_train, W1, b1, W2, b2, W3, b3, num_iterations=1000)


plt.plot(loss_values)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Training Loss for Digits Dataset')
plt.show()

def accuracy(y_true, y_pred):
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return correct / total


def precision(y_true, y_pred):
    true_positive = np.sum(np.logical_and(y_true == 1, y_pred == 1))
    false_positive = np.sum(np.logical_and(y_true == 0, y_pred == 1))
    return true_positive / (true_positive + false_positive)


def recall(y_true, y_pred):
    true_positive = np.sum(np.logical_and(y_true == 1, y_pred == 1))
    false_negative = np.sum(np.logical_and(y_true == 1, y_pred == 0))
    return true_positive / (true_positive + false_negative)


def f1_score(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return 2 * (p * r) / (p + r)



Z1_test = np.dot(X_digits_test, W1) + b1
A1_test = np.tanh(Z1_test)
Z2_test = np.dot(A1_test, W2) + b2
A2_test = 1 / (1 + np.exp(-Z2_test))
predictions = (A2_test > 0.5).astype(int)

# Виклики функцій для обчислення метрик
accuracy = accuracy(Y_digits_test, predictions)
sensitivity = precision(Y_digits_test, predictions)
specificity = recall(Y_digits_test, predictions)
f1_score = f1_score(Y_digits_test, predictions)

print("Accuracy:", accuracy)
print("Precision:", sensitivity)
print("Recall:", specificity)
print("F1-score:", f1_score, "\n")

"""Оригінальний підхід:

Accuracy: 0.164

Precision: 0.933

Recall: 1.0

F1-score: 0.966

Модифікований підхід:

Accuracy: 3.2

Precision: 0.636

Recall: 0.25

F1-score: 0.359
Результати оцінки двох підходів до моделювання можна проаналізувати наступним чином:

<b>Accuracy (Точність):</b>

Оригінальний підхід має низький рівень точності (16.4%), що означає, що велика частина передбачень моделі є неправильними.
Модифікований підхід демонструє надзвичайно високий рівень точності (320%), але це значення є аномальним і, імовірно, є результатом помилки або некоректного обчислення.

<b>Precision (Точність визначення позитивного класу):</b>

Оригінальний підхід має високий рівень точності (93.3%) при визначенні позитивного класу, що свідчить про те, що більшість визначень позитивного класу є правильними.
Модифікований підхід також має високий рівень точності (63.6%), але він нижчий порівняно з оригінальним підходом.

<b>Recall (Чутливість визначення позитивного класу):</b>

Оригінальний підхід має високий рівень чутливості (100%), що означає, що модель правильно виявляє всі позитивні випадки.
Модифікований підхід має нижчий рівень чутливості (25%), що свідчить про те, що він виявляє лише частину позитивних випадків.

</b>F1-score (F1-оцінка):</b>
Оригінальний підхід має високий баланс між точністю та повнотою (96.6%) в порівнянні з модифікованим підходом (35.9%), який має менший баланс між цими показниками.

Узагальнюючи, оригінальний підхід досягає більш високих показників точності, точності визначення позитивного класу та F1-оцінки порівняно з модифікованим підходом. Модифікований підхід має високий рівень чутливості, але низьку точність та F1-оцінку, що може свідчити про його низьку специфічність та велику кількість неправильних позитивних визначень.

# 4. Проекспериментувати з параметрами навчання. Змінити швидкість навчання, кількість епох або інші параметри і проаналізувати, як це впливає на процес навчання і якість отриманих прогнозів.
# 5. Візуалізація результатів. Побудувати графіки, які демонструють процес навчання (наприклад, зменшення помилки втрати від епохи до епохи) або візуалізують вихід мережі на тестовому наборі даних.
# 6. Аналіз помилок. Подивіться на приклади, які мережа класифікувала неправильно. Чи можна визначити якісь закономірності в цих помилках? Це може допомогти вам зрозуміти, як покращити вашу модель.
"""

import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier


digits = datasets.load_digits()
X_digits = digits.data
Y_digits = digits.target.reshape(-1, 1)
scaler_digits = StandardScaler().fit(X_digits)
X_digits = scaler_digits.transform(X_digits)

X_digits_train, X_digits_test, Y_digits_train, Y_digits_test = train_test_split(X_digits, Y_digits, test_size=0.2,
                                                                                random_state=42)


def initialize_weights(n_input, n_hidden, n_output):
    W1 = np.random.randn(n_input, n_hidden) * 0.01
    b1 = np.zeros((1, n_hidden))
    W2 = np.random.randn(n_hidden, n_output) * 0.01
    b2 = np.zeros((1, n_output))

    return W1, b1, W2, b2


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def sigmoid_derivative(x):
    return x * (1 - x)


def initialize_weights(n_input, n_hidden, n_output):
    W1 = np.random.randn(n_input, n_hidden) * 0.01
    b1 = np.zeros((1, n_hidden))
    W2 = np.random.randn(n_hidden, n_output) * 0.01
    b2 = np.zeros((1, n_output))
    return W1, b1, W2, b2


def forward_pass_multiclass(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    return Z1, A1, Z2, A2


def backward_pass_multiclass(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate=0.01):
    m = X.shape[0]
    dZ2 = A2 - Y
    dW2 = 1 / m * np.dot(A1.T, dZ2)
    db2 = 1 / m * np.sum(dZ2, axis=0, keepdims=True)
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * sigmoid_derivative(A1)
    dW1 = 1 / m * np.dot(X.T, dZ1)
    db1 = 1 / m * np.sum(dZ1, axis=0, keepdims=True)
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    return W1, b1, W2, b2


def compute_multiclass_loss(Y, Y_hat):
    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[0]
    L = -(1 / m) * L_sum
    return L


def train_model(X, Y, W1, b1, W2, b2, num_iterations=1000, learning_rate=0.01):
    loss_values = []
    for i in range(num_iterations):
        Z1, A1, Z2, A2 = forward_pass_multiclass(X, W1, b1, W2, b2)
        W1, b1, W2, b2 = backward_pass_multiclass(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate=0.01)
        loss = compute_multiclass_loss(Y, A2)
        loss_values.append(loss)
    return W1, b1, W2, b2, loss_values



learning_rates = [0.1, 0.01, 0.001, 0.0001]
num_lr = len(learning_rates)
fig, axs = plt.subplots(1, num_lr, figsize=(15, 5))

for i, lr in enumerate(learning_rates):
  W1_initial, b1_initial, W2_initial, b2_initial = initialize_weights(X_digits_train.shape[1], n_hidden=5, n_output=1)
  W1, b1, W2, b2, loss_values = train_model(X_digits_train, Y_digits_train, W1_initial, b1_initial, W2_initial, b2_initial, num_iterations=1000, learning_rate=lr)

  axs[i].plot(loss_values)
  axs[i].set_xlabel('Iterations')
  axs[i].set_ylabel('Loss')
  axs[i].set_title(f'Training Loss (LR={lr})')

plt.tight_layout()
plt.show()


epochs_list = [1000, 2000, 3000, 4000, 5000]
num_epochs = len(epochs_list)
fig, axs = plt.subplots(1, num_epochs, figsize=(15, 5))

for i, epochs in enumerate(epochs_list):
  W1_initial, b1_initial, W2_initial, b2_initial = initialize_weights(X_digits_train.shape[1], n_hidden=5, n_output=1)
  W1, b1, W2, b2, loss_values = train_model(X_digits_train, Y_digits_train, W1_initial, b1_initial, W2_initial, b2_initial, num_iterations=epochs)

  axs[i].plot(loss_values)
  axs[i].set_xlabel('Iterations')
  axs[i].set_ylabel('Loss')
  axs[i].set_title(f'Training Loss ({epochs} epochs)')

plt.tight_layout()
plt.show()