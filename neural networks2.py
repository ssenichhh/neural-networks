# -*- coding: utf-8 -*-
"""Практична робота №2 Ащимов Арсеній.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZN8ha4Oai41eo1V2xtRcF1_rA1JC4Pdl

# ПРАКТИЧНА РОБОТА 2: МЕТОДИ ОПТИМІЗАЦІЇ
"""

import numpy as np
import random

"""Градієнтний спуск"""

# Функція для обчислення значення y = 4*sqrt(x) + noise
def compute_y(x):
    noise = random.uniform(-0.1, 0.1)  # Випадковий шум
    return 4 * np.sqrt(x) + noise
# Функція для обчислення похідної (градієнта) функції по x
def compute_gradient(x):
    return 2 / np.sqrt(x)

def gradient_descent(learning_rate, num_iterations):
    x = random.uniform(0.01, 10)  # Початкове значення x
    for i in range(num_iterations):
        gradient = compute_gradient(x)  # Градієнт функції
        x -= learning_rate * gradient  # Оновлення x
        x = max(x, 0.0001)  # Переконатися, що x не стає від'ємним
    return x


# Параметри градієнтного спуску
learning_rate = 0.01
num_iterations = 1000

# Знаходження мінімуму функції
result = gradient_descent(learning_rate, num_iterations)
print("Мінімум функції знаходиться при x =", result)
print("Значення функції в цій точці y =", compute_y(result))

"""Метод Hill Climbing"""

def func(x):
    if x < 0:
        x = 0.001
    noise = random.uniform(-0.1, 0.1)
    return 4 * np.sqrt(x) + noise

def hill_climbing(func, x_start, step = 0.1, max_iterations = 1000):
  x = x_start
  for i in range(max_iterations):
      dx = np.random.normal(0.01, step)
      if func(x + dx) > func(x):
          x = x + dx
  return x

x_start = 0.1

optimal_x = hill_climbing(func, x_start)

print("Optimal value x:", optimal_x)

"""# Генетичний алгоритм"""

np.random.seed(42)
sz = 1000
x = np.random.rand(sz, 1)
y = 4 * np.sqrt(x) + np.random.normal(0.0, 0.1)

idx = np.arange(sz)
np.random.shuffle(idx)
x_train, y_train = x[idx], y[idx]

# Початкові випадкові значення для a і b
a_initial = np.random.randn(1)
b_initial = np.random.randn(1)

# Розрахунок початкової помилки
initial_error = ((y_train - (a_initial + b_initial * x_train)) ** 2).mean()
print(f"Початкова помилка: {initial_error}")
population_size = 100
num_generations = 1000
mutation_rate = 0.01

# Створюємо початкову популяцію
population = np.random.randn(population_size, 2)

for _ in range(num_generations):
  # Обчислюємо помилку для кожного індивіда в популяції
  errors = np.array([((y_train - (a + b * x_train)) ** 2).mean() for a, b in population])

  # Вибираємо найкращих індивідів (тих, у кого помилка найменша)
  fitness_scores = 1 / (1 + errors)
  probs = fitness_scores / fitness_scores.sum()
  selected = population[np.random.choice(np.arange(population_size), size = population_size, replace = True, p = probs)]

  # Створюємо нове покоління через схрещування
  pairs = selected[np.random.randint(0, population_size, size = (population_size, 2))]
  new_population = pairs.mean(axis = 1)

  # Застосовуємо мутації
  mutations = (np.random.rand(population_size, 2) - 0.5) * mutation_rate
  new_population += mutations

  # Замінюємо стару популяцію новою
  population = new_population

# Вибираємо найкраще рішення з кінцевої популяції
best_idx = np.argmin([((y_train - (a + b * x_train)) ** 2).mean() for a, b in population])
best_a, best_b = population[best_idx]

# Розрахунок помилки після оптимізації
final_error = ((y_train - (best_a + best_b * x_train)) ** 2).mean()
print(f"Кінцева помилка: {final_error}")